{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5543b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1c676b",
   "metadata": {},
   "source": [
    "# Reading Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f0daa5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout. The point of using Lorem Ipsum is that it has a more-or-less normal distribution of letters, as opposed to using 'Content here, content here', making it look like readable English. Many desktop publishing packages and web page editors now use Lorem Ipsum as their default model text, and a search for 'lorem ipsum' will uncover many web sites still in their infancy. Various versions have evolved over the years, sometimes by accident, sometimes on purpose (injected humour and the like).There are many variations of passages of Lorem Ipsum available, but the majority have suffered alteration in some form, by injected humour, or randomised words which don't look even slightly believable. If you are going to use a passage of Lorem Ipsum, you need to be sure there isn't anything embarrassing hidden in the middle of text. All the Lorem Ipsum generators on the Internet tend to repeat predefined chunks as necessary, making this the first true generator on the Internet. It uses a dictionary of over 200 Latin words, combined with a handful of model sentence structures, to generate Lorem Ipsum which looks reasonable. The generated Lorem Ipsum is therefore always free from repetition, injected humour, or non-characteristic words etc.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading text file content\n",
    "with open(\"sample.txt\",'r') as obj:\n",
    "    paragraph=obj.read()\n",
    "paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb60c81",
   "metadata": {},
   "source": [
    "# Downloading punkt from nltk to tokenize word and sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a06f0f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Shahana\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To use punkt\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d989036f",
   "metadata": {},
   "source": [
    "# Tokenizing paragraph into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "695058a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry.',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.\",\n",
       " 'It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.',\n",
       " 'It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout.',\n",
       " \"The point of using Lorem Ipsum is that it has a more-or-less normal distribution of letters, as opposed to using 'Content here, content here', making it look like readable English.\",\n",
       " \"Many desktop publishing packages and web page editors now use Lorem Ipsum as their default model text, and a search for 'lorem ipsum' will uncover many web sites still in their infancy.\",\n",
       " \"Various versions have evolved over the years, sometimes by accident, sometimes on purpose (injected humour and the like).There are many variations of passages of Lorem Ipsum available, but the majority have suffered alteration in some form, by injected humour, or randomised words which don't look even slightly believable.\",\n",
       " \"If you are going to use a passage of Lorem Ipsum, you need to be sure there isn't anything embarrassing hidden in the middle of text.\",\n",
       " 'All the Lorem Ipsum generators on the Internet tend to repeat predefined chunks as necessary, making this the first true generator on the Internet.',\n",
       " 'It uses a dictionary of over 200 Latin words, combined with a handful of model sentence structures, to generate Lorem Ipsum which looks reasonable.',\n",
       " 'The generated Lorem Ipsum is therefore always free from repetition, injected humour, or non-characteristic words etc.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#breaking paragraph into sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentence_list=sent_tokenize(paragraph)\n",
    "sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f67bb2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry.',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.\",\n",
       " 'It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.',\n",
       " 'It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout.',\n",
       " \"The point of using Lorem Ipsum is that it has a more-or-less normal distribution of letters, as opposed to using 'Content here, content here', making it look like readable English.\",\n",
       " \"Many desktop publishing packages and web page editors now use Lorem Ipsum as their default model text, and a search for 'lorem ipsum' will uncover many web sites still in their infancy.\",\n",
       " \"Various versions have evolved over the years, sometimes by accident, sometimes on purpose (injected humour and the like).There are many variations of passages of Lorem Ipsum available, but the majority have suffered alteration in some form, by injected humour, or randomised words which don't look even slightly believable.\",\n",
       " \"If you are going to use a passage of Lorem Ipsum, you need to be sure there isn't anything embarrassing hidden in the middle of text.\",\n",
       " 'All the Lorem Ipsum generators on the Internet tend to repeat predefined chunks as necessary, making this the first true generator on the Internet.',\n",
       " 'It uses a dictionary of over 200 Latin words, combined with a handful of model sentence structures, to generate Lorem Ipsum which looks reasonable.',\n",
       " 'The generated Lorem Ipsum is therefore always free from repetition, injected humour, or non-characteristic words etc.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "sent_obj=PunktSentenceTokenizer(text)\n",
    "sent_obj.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c3238c",
   "metadata": {},
   "source": [
    "# Tokenizing sentence into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "898a92c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Lorem',\n",
       "  'Ipsum',\n",
       "  'is',\n",
       "  'simply',\n",
       "  'dummy',\n",
       "  'text',\n",
       "  'of',\n",
       "  'the',\n",
       "  'printing',\n",
       "  'and',\n",
       "  'typesetting',\n",
       "  'industry',\n",
       "  '.'],\n",
       " ['Lorem',\n",
       "  'Ipsum',\n",
       "  'has',\n",
       "  'been',\n",
       "  'the',\n",
       "  'industry',\n",
       "  \"'s\",\n",
       "  'standard',\n",
       "  'dummy',\n",
       "  'text',\n",
       "  'ever',\n",
       "  'since',\n",
       "  'the',\n",
       "  '1500s',\n",
       "  ',',\n",
       "  'when',\n",
       "  'an',\n",
       "  'unknown',\n",
       "  'printer',\n",
       "  'took',\n",
       "  'a',\n",
       "  'galley',\n",
       "  'of',\n",
       "  'type',\n",
       "  'and',\n",
       "  'scrambled',\n",
       "  'it',\n",
       "  'to',\n",
       "  'make',\n",
       "  'a',\n",
       "  'type',\n",
       "  'specimen',\n",
       "  'book',\n",
       "  '.'],\n",
       " ['It',\n",
       "  'has',\n",
       "  'survived',\n",
       "  'not',\n",
       "  'only',\n",
       "  'five',\n",
       "  'centuries',\n",
       "  ',',\n",
       "  'but',\n",
       "  'also',\n",
       "  'the',\n",
       "  'leap',\n",
       "  'into',\n",
       "  'electronic',\n",
       "  'typesetting',\n",
       "  ',',\n",
       "  'remaining',\n",
       "  'essentially',\n",
       "  'unchanged',\n",
       "  '.'],\n",
       " ['It',\n",
       "  'was',\n",
       "  'popularised',\n",
       "  'in',\n",
       "  'the',\n",
       "  '1960s',\n",
       "  'with',\n",
       "  'the',\n",
       "  'release',\n",
       "  'of',\n",
       "  'Letraset',\n",
       "  'sheets',\n",
       "  'containing',\n",
       "  'Lorem',\n",
       "  'Ipsum',\n",
       "  'passages',\n",
       "  ',',\n",
       "  'and',\n",
       "  'more',\n",
       "  'recently',\n",
       "  'with',\n",
       "  'desktop',\n",
       "  'publishing',\n",
       "  'software',\n",
       "  'like',\n",
       "  'Aldus',\n",
       "  'PageMaker',\n",
       "  'including',\n",
       "  'versions',\n",
       "  'of',\n",
       "  'Lorem',\n",
       "  'Ipsum.It',\n",
       "  'is',\n",
       "  'a',\n",
       "  'long',\n",
       "  'established',\n",
       "  'fact',\n",
       "  'that',\n",
       "  'a',\n",
       "  'reader',\n",
       "  'will',\n",
       "  'be',\n",
       "  'distracted',\n",
       "  'by',\n",
       "  'the',\n",
       "  'readable',\n",
       "  'content',\n",
       "  'of',\n",
       "  'a',\n",
       "  'page',\n",
       "  'when',\n",
       "  'looking',\n",
       "  'at',\n",
       "  'its',\n",
       "  'layout',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'point',\n",
       "  'of',\n",
       "  'using',\n",
       "  'Lorem',\n",
       "  'Ipsum',\n",
       "  'is',\n",
       "  'that',\n",
       "  'it',\n",
       "  'has',\n",
       "  'a',\n",
       "  'more-or-less',\n",
       "  'normal',\n",
       "  'distribution',\n",
       "  'of',\n",
       "  'letters',\n",
       "  ',',\n",
       "  'as',\n",
       "  'opposed',\n",
       "  'to',\n",
       "  'using',\n",
       "  \"'Content\",\n",
       "  'here',\n",
       "  ',',\n",
       "  'content',\n",
       "  'here',\n",
       "  \"'\",\n",
       "  ',',\n",
       "  'making',\n",
       "  'it',\n",
       "  'look',\n",
       "  'like',\n",
       "  'readable',\n",
       "  'English',\n",
       "  '.'],\n",
       " ['Many',\n",
       "  'desktop',\n",
       "  'publishing',\n",
       "  'packages',\n",
       "  'and',\n",
       "  'web',\n",
       "  'page',\n",
       "  'editors',\n",
       "  'now',\n",
       "  'use',\n",
       "  'Lorem',\n",
       "  'Ipsum',\n",
       "  'as',\n",
       "  'their',\n",
       "  'default',\n",
       "  'model',\n",
       "  'text',\n",
       "  ',',\n",
       "  'and',\n",
       "  'a',\n",
       "  'search',\n",
       "  'for',\n",
       "  \"'lorem\",\n",
       "  'ipsum',\n",
       "  \"'\",\n",
       "  'will',\n",
       "  'uncover',\n",
       "  'many',\n",
       "  'web',\n",
       "  'sites',\n",
       "  'still',\n",
       "  'in',\n",
       "  'their',\n",
       "  'infancy',\n",
       "  '.'],\n",
       " ['Various',\n",
       "  'versions',\n",
       "  'have',\n",
       "  'evolved',\n",
       "  'over',\n",
       "  'the',\n",
       "  'years',\n",
       "  ',',\n",
       "  'sometimes',\n",
       "  'by',\n",
       "  'accident',\n",
       "  ',',\n",
       "  'sometimes',\n",
       "  'on',\n",
       "  'purpose',\n",
       "  '(',\n",
       "  'injected',\n",
       "  'humour',\n",
       "  'and',\n",
       "  'the',\n",
       "  'like',\n",
       "  ')',\n",
       "  '.There',\n",
       "  'are',\n",
       "  'many',\n",
       "  'variations',\n",
       "  'of',\n",
       "  'passages',\n",
       "  'of',\n",
       "  'Lorem',\n",
       "  'Ipsum',\n",
       "  'available',\n",
       "  ',',\n",
       "  'but',\n",
       "  'the',\n",
       "  'majority',\n",
       "  'have',\n",
       "  'suffered',\n",
       "  'alteration',\n",
       "  'in',\n",
       "  'some',\n",
       "  'form',\n",
       "  ',',\n",
       "  'by',\n",
       "  'injected',\n",
       "  'humour',\n",
       "  ',',\n",
       "  'or',\n",
       "  'randomised',\n",
       "  'words',\n",
       "  'which',\n",
       "  'do',\n",
       "  \"n't\",\n",
       "  'look',\n",
       "  'even',\n",
       "  'slightly',\n",
       "  'believable',\n",
       "  '.'],\n",
       " ['If',\n",
       "  'you',\n",
       "  'are',\n",
       "  'going',\n",
       "  'to',\n",
       "  'use',\n",
       "  'a',\n",
       "  'passage',\n",
       "  'of',\n",
       "  'Lorem',\n",
       "  'Ipsum',\n",
       "  ',',\n",
       "  'you',\n",
       "  'need',\n",
       "  'to',\n",
       "  'be',\n",
       "  'sure',\n",
       "  'there',\n",
       "  'is',\n",
       "  \"n't\",\n",
       "  'anything',\n",
       "  'embarrassing',\n",
       "  'hidden',\n",
       "  'in',\n",
       "  'the',\n",
       "  'middle',\n",
       "  'of',\n",
       "  'text',\n",
       "  '.'],\n",
       " ['All',\n",
       "  'the',\n",
       "  'Lorem',\n",
       "  'Ipsum',\n",
       "  'generators',\n",
       "  'on',\n",
       "  'the',\n",
       "  'Internet',\n",
       "  'tend',\n",
       "  'to',\n",
       "  'repeat',\n",
       "  'predefined',\n",
       "  'chunks',\n",
       "  'as',\n",
       "  'necessary',\n",
       "  ',',\n",
       "  'making',\n",
       "  'this',\n",
       "  'the',\n",
       "  'first',\n",
       "  'true',\n",
       "  'generator',\n",
       "  'on',\n",
       "  'the',\n",
       "  'Internet',\n",
       "  '.'],\n",
       " ['It',\n",
       "  'uses',\n",
       "  'a',\n",
       "  'dictionary',\n",
       "  'of',\n",
       "  'over',\n",
       "  '200',\n",
       "  'Latin',\n",
       "  'words',\n",
       "  ',',\n",
       "  'combined',\n",
       "  'with',\n",
       "  'a',\n",
       "  'handful',\n",
       "  'of',\n",
       "  'model',\n",
       "  'sentence',\n",
       "  'structures',\n",
       "  ',',\n",
       "  'to',\n",
       "  'generate',\n",
       "  'Lorem',\n",
       "  'Ipsum',\n",
       "  'which',\n",
       "  'looks',\n",
       "  'reasonable',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'generated',\n",
       "  'Lorem',\n",
       "  'Ipsum',\n",
       "  'is',\n",
       "  'therefore',\n",
       "  'always',\n",
       "  'free',\n",
       "  'from',\n",
       "  'repetition',\n",
       "  ',',\n",
       "  'injected',\n",
       "  'humour',\n",
       "  ',',\n",
       "  'or',\n",
       "  'non-characteristic',\n",
       "  'words',\n",
       "  'etc',\n",
       "  '.']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "ll=[]  #lisi of list in which each list contains words of a sentence as tokens\n",
    "for sentence in sentence_list:\n",
    "    ll.append(nltk.word_tokenize(sentence))\n",
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "304505a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['won', '’', 't', 'go']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.word_tokenize(\"won’t go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecc79894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem',\n",
       " 'Ipsum',\n",
       " 'is',\n",
       " 'simply',\n",
       " 'dummy',\n",
       " 'text',\n",
       " 'of',\n",
       " 'the',\n",
       " 'printing',\n",
       " 'and',\n",
       " 'typesetting',\n",
       " 'industry',\n",
       " '.',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'has',\n",
       " 'been',\n",
       " 'the',\n",
       " 'industry',\n",
       " \"'\",\n",
       " 's',\n",
       " 'standard',\n",
       " 'dummy',\n",
       " 'text',\n",
       " 'ever',\n",
       " 'since',\n",
       " 'the',\n",
       " '1500s',\n",
       " ',',\n",
       " 'when',\n",
       " 'an',\n",
       " 'unknown',\n",
       " 'printer',\n",
       " 'took',\n",
       " 'a',\n",
       " 'galley',\n",
       " 'of',\n",
       " 'type',\n",
       " 'and',\n",
       " 'scrambled',\n",
       " 'it',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'type',\n",
       " 'specimen',\n",
       " 'book',\n",
       " '.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'survived',\n",
       " 'not',\n",
       " 'only',\n",
       " 'five',\n",
       " 'centuries',\n",
       " ',',\n",
       " 'but',\n",
       " 'also',\n",
       " 'the',\n",
       " 'leap',\n",
       " 'into',\n",
       " 'electronic',\n",
       " 'typesetting',\n",
       " ',',\n",
       " 'remaining',\n",
       " 'essentially',\n",
       " 'unchanged',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'popularised',\n",
       " 'in',\n",
       " 'the',\n",
       " '1960s',\n",
       " 'with',\n",
       " 'the',\n",
       " 'release',\n",
       " 'of',\n",
       " 'Letraset',\n",
       " 'sheets',\n",
       " 'containing',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'passages',\n",
       " ',',\n",
       " 'and',\n",
       " 'more',\n",
       " 'recently',\n",
       " 'with',\n",
       " 'desktop',\n",
       " 'publishing',\n",
       " 'software',\n",
       " 'like',\n",
       " 'Aldus',\n",
       " 'PageMaker',\n",
       " 'including',\n",
       " 'versions',\n",
       " 'of',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'a',\n",
       " 'long',\n",
       " 'established',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'a',\n",
       " 'reader',\n",
       " 'will',\n",
       " 'be',\n",
       " 'distracted',\n",
       " 'by',\n",
       " 'the',\n",
       " 'readable',\n",
       " 'content',\n",
       " 'of',\n",
       " 'a',\n",
       " 'page',\n",
       " 'when',\n",
       " 'looking',\n",
       " 'at',\n",
       " 'its',\n",
       " 'layout',\n",
       " '.',\n",
       " 'The',\n",
       " 'point',\n",
       " 'of',\n",
       " 'using',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'is',\n",
       " 'that',\n",
       " 'it',\n",
       " 'has',\n",
       " 'a',\n",
       " 'more',\n",
       " '-',\n",
       " 'or',\n",
       " '-',\n",
       " 'less',\n",
       " 'normal',\n",
       " 'distribution',\n",
       " 'of',\n",
       " 'letters',\n",
       " ',',\n",
       " 'as',\n",
       " 'opposed',\n",
       " 'to',\n",
       " 'using',\n",
       " \"'\",\n",
       " 'Content',\n",
       " 'here',\n",
       " ',',\n",
       " 'content',\n",
       " 'here',\n",
       " \"',\",\n",
       " 'making',\n",
       " 'it',\n",
       " 'look',\n",
       " 'like',\n",
       " 'readable',\n",
       " 'English',\n",
       " '.',\n",
       " 'Many',\n",
       " 'desktop',\n",
       " 'publishing',\n",
       " 'packages',\n",
       " 'and',\n",
       " 'web',\n",
       " 'page',\n",
       " 'editors',\n",
       " 'now',\n",
       " 'use',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'as',\n",
       " 'their',\n",
       " 'default',\n",
       " 'model',\n",
       " 'text',\n",
       " ',',\n",
       " 'and',\n",
       " 'a',\n",
       " 'search',\n",
       " 'for',\n",
       " \"'\",\n",
       " 'lorem',\n",
       " 'ipsum',\n",
       " \"'\",\n",
       " 'will',\n",
       " 'uncover',\n",
       " 'many',\n",
       " 'web',\n",
       " 'sites',\n",
       " 'still',\n",
       " 'in',\n",
       " 'their',\n",
       " 'infancy',\n",
       " '.',\n",
       " 'Various',\n",
       " 'versions',\n",
       " 'have',\n",
       " 'evolved',\n",
       " 'over',\n",
       " 'the',\n",
       " 'years',\n",
       " ',',\n",
       " 'sometimes',\n",
       " 'by',\n",
       " 'accident',\n",
       " ',',\n",
       " 'sometimes',\n",
       " 'on',\n",
       " 'purpose',\n",
       " '(',\n",
       " 'injected',\n",
       " 'humour',\n",
       " 'and',\n",
       " 'the',\n",
       " 'like',\n",
       " ').',\n",
       " 'There',\n",
       " 'are',\n",
       " 'many',\n",
       " 'variations',\n",
       " 'of',\n",
       " 'passages',\n",
       " 'of',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'available',\n",
       " ',',\n",
       " 'but',\n",
       " 'the',\n",
       " 'majority',\n",
       " 'have',\n",
       " 'suffered',\n",
       " 'alteration',\n",
       " 'in',\n",
       " 'some',\n",
       " 'form',\n",
       " ',',\n",
       " 'by',\n",
       " 'injected',\n",
       " 'humour',\n",
       " ',',\n",
       " 'or',\n",
       " 'randomised',\n",
       " 'words',\n",
       " 'which',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'look',\n",
       " 'even',\n",
       " 'slightly',\n",
       " 'believable',\n",
       " '.',\n",
       " 'If',\n",
       " 'you',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'use',\n",
       " 'a',\n",
       " 'passage',\n",
       " 'of',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " ',',\n",
       " 'you',\n",
       " 'need',\n",
       " 'to',\n",
       " 'be',\n",
       " 'sure',\n",
       " 'there',\n",
       " 'isn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'anything',\n",
       " 'embarrassing',\n",
       " 'hidden',\n",
       " 'in',\n",
       " 'the',\n",
       " 'middle',\n",
       " 'of',\n",
       " 'text',\n",
       " '.',\n",
       " 'All',\n",
       " 'the',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'generators',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Internet',\n",
       " 'tend',\n",
       " 'to',\n",
       " 'repeat',\n",
       " 'predefined',\n",
       " 'chunks',\n",
       " 'as',\n",
       " 'necessary',\n",
       " ',',\n",
       " 'making',\n",
       " 'this',\n",
       " 'the',\n",
       " 'first',\n",
       " 'true',\n",
       " 'generator',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Internet',\n",
       " '.',\n",
       " 'It',\n",
       " 'uses',\n",
       " 'a',\n",
       " 'dictionary',\n",
       " 'of',\n",
       " 'over',\n",
       " '200',\n",
       " 'Latin',\n",
       " 'words',\n",
       " ',',\n",
       " 'combined',\n",
       " 'with',\n",
       " 'a',\n",
       " 'handful',\n",
       " 'of',\n",
       " 'model',\n",
       " 'sentence',\n",
       " 'structures',\n",
       " ',',\n",
       " 'to',\n",
       " 'generate',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'which',\n",
       " 'looks',\n",
       " 'reasonable',\n",
       " '.',\n",
       " 'The',\n",
       " 'generated',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'is',\n",
       " 'therefore',\n",
       " 'always',\n",
       " 'free',\n",
       " 'from',\n",
       " 'repetition',\n",
       " ',',\n",
       " 'injected',\n",
       " 'humour',\n",
       " ',',\n",
       " 'or',\n",
       " 'non',\n",
       " '-',\n",
       " 'characteristic',\n",
       " 'words',\n",
       " 'etc',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_obj=WordPunctTokenizer()\n",
    "tokens=word_obj.tokenize(paragraph)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbe0798d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'can', \"'\", 't', 'allow', 'you', 'to', 'go', 'home', 'early']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(\" I can't allow you to go home early\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccb79d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"can't\", 'is', 'a', 'contraction']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokenizer.tokenize(\"can't is a contraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3e0b3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"won't\", 'is', 'a', 'contraction']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"won't is a contraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117fcaca",
   "metadata": {},
   "source": [
    "# Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d177911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Shahana\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe0918cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#listing stopwords in english\n",
    "from nltk.corpus import stopwords\n",
    "eng_stopwords=set(stopwords.words('english'))\n",
    "eng_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "75c73b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem',\n",
       " 'Ipsum',\n",
       " 'simply',\n",
       " 'dummy',\n",
       " 'text',\n",
       " 'printing',\n",
       " 'typesetting',\n",
       " 'industry',\n",
       " '.',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'industry',\n",
       " \"'\",\n",
       " 'standard',\n",
       " 'dummy',\n",
       " 'text',\n",
       " 'ever',\n",
       " 'since',\n",
       " '1500s',\n",
       " ',',\n",
       " 'unknown',\n",
       " 'printer',\n",
       " 'took',\n",
       " 'galley',\n",
       " 'type',\n",
       " 'scrambled',\n",
       " 'make',\n",
       " 'type',\n",
       " 'specimen',\n",
       " 'book',\n",
       " '.',\n",
       " 'It',\n",
       " 'survived',\n",
       " 'five',\n",
       " 'centuries',\n",
       " ',',\n",
       " 'also',\n",
       " 'leap',\n",
       " 'electronic',\n",
       " 'typesetting',\n",
       " ',',\n",
       " 'remaining',\n",
       " 'essentially',\n",
       " 'unchanged',\n",
       " '.',\n",
       " 'It',\n",
       " 'popularised',\n",
       " '1960s',\n",
       " 'release',\n",
       " 'Letraset',\n",
       " 'sheets',\n",
       " 'containing',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'passages',\n",
       " ',',\n",
       " 'recently',\n",
       " 'desktop',\n",
       " 'publishing',\n",
       " 'software',\n",
       " 'like',\n",
       " 'Aldus',\n",
       " 'PageMaker',\n",
       " 'including',\n",
       " 'versions',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " '.',\n",
       " 'It',\n",
       " 'long',\n",
       " 'established',\n",
       " 'fact',\n",
       " 'reader',\n",
       " 'distracted',\n",
       " 'readable',\n",
       " 'content',\n",
       " 'page',\n",
       " 'looking',\n",
       " 'layout',\n",
       " '.',\n",
       " 'The',\n",
       " 'point',\n",
       " 'using',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " '-',\n",
       " '-',\n",
       " 'less',\n",
       " 'normal',\n",
       " 'distribution',\n",
       " 'letters',\n",
       " ',',\n",
       " 'opposed',\n",
       " 'using',\n",
       " \"'\",\n",
       " 'Content',\n",
       " ',',\n",
       " 'content',\n",
       " \"',\",\n",
       " 'making',\n",
       " 'look',\n",
       " 'like',\n",
       " 'readable',\n",
       " 'English',\n",
       " '.',\n",
       " 'Many',\n",
       " 'desktop',\n",
       " 'publishing',\n",
       " 'packages',\n",
       " 'web',\n",
       " 'page',\n",
       " 'editors',\n",
       " 'use',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'default',\n",
       " 'model',\n",
       " 'text',\n",
       " ',',\n",
       " 'search',\n",
       " \"'\",\n",
       " 'lorem',\n",
       " 'ipsum',\n",
       " \"'\",\n",
       " 'uncover',\n",
       " 'many',\n",
       " 'web',\n",
       " 'sites',\n",
       " 'still',\n",
       " 'infancy',\n",
       " '.',\n",
       " 'Various',\n",
       " 'versions',\n",
       " 'evolved',\n",
       " 'years',\n",
       " ',',\n",
       " 'sometimes',\n",
       " 'accident',\n",
       " ',',\n",
       " 'sometimes',\n",
       " 'purpose',\n",
       " '(',\n",
       " 'injected',\n",
       " 'humour',\n",
       " 'like',\n",
       " ').',\n",
       " 'There',\n",
       " 'many',\n",
       " 'variations',\n",
       " 'passages',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'available',\n",
       " ',',\n",
       " 'majority',\n",
       " 'suffered',\n",
       " 'alteration',\n",
       " 'form',\n",
       " ',',\n",
       " 'injected',\n",
       " 'humour',\n",
       " ',',\n",
       " 'randomised',\n",
       " 'words',\n",
       " \"'\",\n",
       " 'look',\n",
       " 'even',\n",
       " 'slightly',\n",
       " 'believable',\n",
       " '.',\n",
       " 'If',\n",
       " 'going',\n",
       " 'use',\n",
       " 'passage',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " ',',\n",
       " 'need',\n",
       " 'sure',\n",
       " \"'\",\n",
       " 'anything',\n",
       " 'embarrassing',\n",
       " 'hidden',\n",
       " 'middle',\n",
       " 'text',\n",
       " '.',\n",
       " 'All',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'generators',\n",
       " 'Internet',\n",
       " 'tend',\n",
       " 'repeat',\n",
       " 'predefined',\n",
       " 'chunks',\n",
       " 'necessary',\n",
       " ',',\n",
       " 'making',\n",
       " 'first',\n",
       " 'true',\n",
       " 'generator',\n",
       " 'Internet',\n",
       " '.',\n",
       " 'It',\n",
       " 'uses',\n",
       " 'dictionary',\n",
       " '200',\n",
       " 'Latin',\n",
       " 'words',\n",
       " ',',\n",
       " 'combined',\n",
       " 'handful',\n",
       " 'model',\n",
       " 'sentence',\n",
       " 'structures',\n",
       " ',',\n",
       " 'generate',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'looks',\n",
       " 'reasonable',\n",
       " '.',\n",
       " 'The',\n",
       " 'generated',\n",
       " 'Lorem',\n",
       " 'Ipsum',\n",
       " 'therefore',\n",
       " 'always',\n",
       " 'free',\n",
       " 'repetition',\n",
       " ',',\n",
       " 'injected',\n",
       " 'humour',\n",
       " ',',\n",
       " 'non',\n",
       " '-',\n",
       " 'characteristic',\n",
       " 'words',\n",
       " 'etc',\n",
       " '.']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_after_stopwords_removal=[token for token in tokens if token not in eng_stopwords] #After removing stop words\n",
    "tokens_after_stopwords_removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2fab95a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'writer']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_stops = set(stopwords.words('english'))\n",
    "words = ['I', 'am', 'a', 'writer']\n",
    "[word for word in words if word not in english_stops]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2cc7f3",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cc39d6",
   "metadata": {},
   "source": [
    "## What is Stemming?\n",
    "### Stemming is a technique used to extract the base form of the words by removing affixes from them. It is just like cutting down the branches of a tree to its stems. For example, the stem of the words eating, eats, eaten is eat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "75a11904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'write'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "word_stemmer = PorterStemmer()\n",
    "word_stemmer.stem('writing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "34d8d168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lorem',\n",
       " 'ipsum',\n",
       " 'simpli',\n",
       " 'dummi',\n",
       " 'text',\n",
       " 'print',\n",
       " 'typeset',\n",
       " 'industri',\n",
       " '.',\n",
       " 'lorem',\n",
       " 'ipsum',\n",
       " 'industri',\n",
       " \"'\",\n",
       " 'standard',\n",
       " 'dummi',\n",
       " 'text',\n",
       " 'ever',\n",
       " 'sinc',\n",
       " '1500',\n",
       " ',',\n",
       " 'unknown',\n",
       " 'printer',\n",
       " 'took',\n",
       " 'galley',\n",
       " 'type',\n",
       " 'scrambl',\n",
       " 'make',\n",
       " 'type',\n",
       " 'specimen',\n",
       " 'book',\n",
       " '.',\n",
       " 'it',\n",
       " 'surviv',\n",
       " 'five',\n",
       " 'centuri',\n",
       " ',',\n",
       " 'also',\n",
       " 'leap',\n",
       " 'electron',\n",
       " 'typeset',\n",
       " ',',\n",
       " 'remain',\n",
       " 'essenti',\n",
       " 'unchang',\n",
       " '.',\n",
       " 'it',\n",
       " 'popularis',\n",
       " '1960',\n",
       " 'releas',\n",
       " 'letraset',\n",
       " 'sheet',\n",
       " 'contain',\n",
       " 'lorem',\n",
       " 'ipsum',\n",
       " 'passag',\n",
       " ',',\n",
       " 'recent',\n",
       " 'desktop',\n",
       " 'publish',\n",
       " 'softwar',\n",
       " 'like',\n",
       " 'aldu',\n",
       " 'pagemak',\n",
       " 'includ',\n",
       " 'version',\n",
       " 'lorem',\n",
       " 'ipsum',\n",
       " '.',\n",
       " 'it',\n",
       " 'long',\n",
       " 'establish',\n",
       " 'fact',\n",
       " 'reader',\n",
       " 'distract',\n",
       " 'readabl',\n",
       " 'content',\n",
       " 'page',\n",
       " 'look',\n",
       " 'layout',\n",
       " '.',\n",
       " 'the',\n",
       " 'point',\n",
       " 'use',\n",
       " 'lorem',\n",
       " 'ipsum',\n",
       " '-',\n",
       " '-',\n",
       " 'less',\n",
       " 'normal',\n",
       " 'distribut',\n",
       " 'letter',\n",
       " ',',\n",
       " 'oppos',\n",
       " 'use',\n",
       " \"'\",\n",
       " 'content',\n",
       " ',',\n",
       " 'content',\n",
       " \"',\",\n",
       " 'make',\n",
       " 'look',\n",
       " 'like',\n",
       " 'readabl',\n",
       " 'english',\n",
       " '.',\n",
       " 'mani',\n",
       " 'desktop',\n",
       " 'publish',\n",
       " 'packag',\n",
       " 'web',\n",
       " 'page',\n",
       " 'editor',\n",
       " 'use',\n",
       " 'lorem',\n",
       " 'ipsum',\n",
       " 'default',\n",
       " 'model',\n",
       " 'text',\n",
       " ',',\n",
       " 'search',\n",
       " \"'\",\n",
       " 'lorem',\n",
       " 'ipsum',\n",
       " \"'\",\n",
       " 'uncov',\n",
       " 'mani',\n",
       " 'web',\n",
       " 'site',\n",
       " 'still',\n",
       " 'infanc',\n",
       " '.',\n",
       " 'variou',\n",
       " 'version',\n",
       " 'evolv',\n",
       " 'year',\n",
       " ',',\n",
       " 'sometim',\n",
       " 'accid',\n",
       " ',',\n",
       " 'sometim',\n",
       " 'purpos',\n",
       " '(',\n",
       " 'inject',\n",
       " 'humour',\n",
       " 'like',\n",
       " ').',\n",
       " 'there',\n",
       " 'mani',\n",
       " 'variat',\n",
       " 'passag',\n",
       " 'lorem',\n",
       " 'ipsum',\n",
       " 'avail',\n",
       " ',',\n",
       " 'major',\n",
       " 'suffer',\n",
       " 'alter',\n",
       " 'form',\n",
       " ',',\n",
       " 'inject',\n",
       " 'humour',\n",
       " ',',\n",
       " 'randomis',\n",
       " 'word',\n",
       " \"'\",\n",
       " 'look',\n",
       " 'even',\n",
       " 'slightli',\n",
       " 'believ',\n",
       " '.',\n",
       " 'if',\n",
       " 'go',\n",
       " 'use',\n",
       " 'passag',\n",
       " 'lorem',\n",
       " 'ipsum',\n",
       " ',',\n",
       " 'need',\n",
       " 'sure',\n",
       " \"'\",\n",
       " 'anyth',\n",
       " 'embarrass',\n",
       " 'hidden',\n",
       " 'middl',\n",
       " 'text',\n",
       " '.',\n",
       " 'all',\n",
       " 'lorem',\n",
       " 'ipsum',\n",
       " 'gener',\n",
       " 'internet',\n",
       " 'tend',\n",
       " 'repeat',\n",
       " 'predefin',\n",
       " 'chunk',\n",
       " 'necessari',\n",
       " ',',\n",
       " 'make',\n",
       " 'first',\n",
       " 'true',\n",
       " 'gener',\n",
       " 'internet',\n",
       " '.',\n",
       " 'it',\n",
       " 'use',\n",
       " 'dictionari',\n",
       " '200',\n",
       " 'latin',\n",
       " 'word',\n",
       " ',',\n",
       " 'combin',\n",
       " 'hand',\n",
       " 'model',\n",
       " 'sentenc',\n",
       " 'structur',\n",
       " ',',\n",
       " 'gener',\n",
       " 'lorem',\n",
       " 'ipsum',\n",
       " 'look',\n",
       " 'reason',\n",
       " '.',\n",
       " 'the',\n",
       " 'gener',\n",
       " 'lorem',\n",
       " 'ipsum',\n",
       " 'therefor',\n",
       " 'alway',\n",
       " 'free',\n",
       " 'repetit',\n",
       " ',',\n",
       " 'inject',\n",
       " 'humour',\n",
       " ',',\n",
       " 'non',\n",
       " '-',\n",
       " 'characterist',\n",
       " 'word',\n",
       " 'etc',\n",
       " '.']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming=[word_stemmer.stem(i) for i in tokens_after_stopwords_removal]\n",
    "stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1481750",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068521bc",
   "metadata": {},
   "source": [
    "## What is Lemmatization?\n",
    "### Lemmatization technique is like stemming. The output we will get after lemmatization is called ‘lemma’, which is a root word rather than root stem, the output of stemming. After lemmatization, we will be getting a valid word that means the same thing.NLTK provides WordNetLemmatizer class which is a thin wrapper around the wordnet corpus. This class uses morphy() function to the WordNet CorpusReader class to find a lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3be6f165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'belief'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('believes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f23035",
   "metadata": {},
   "source": [
    "## DIfference between stemming and lemmatization ?\n",
    "### stemming technique only looks at the form of the word whereas lemmatization technique looks at the meaning of the word. It means after applying lemmatization, we will always get a valid word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8ffa9d",
   "metadata": {},
   "source": [
    "# Wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7770ba42",
   "metadata": {},
   "source": [
    "### Wordnet is a large lexical database of English, which was created by Princeton. It is a part of the NLTK corpus. Nouns, verbs, adjectives and adverbs all are grouped into set of synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "95d24ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa68ec6",
   "metadata": {},
   "source": [
    "### Synset are groupings of synonyms words that express the same concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9756d2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog.n.01'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn = wn.synsets('dog')[0] #synonym instance\n",
    "syn.name() #method  usedto get the unique name for the synset which can be used to get the Synset directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "98493974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.definition() #gives meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c5a9bfa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the dog barked all night']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c4389e",
   "metadata": {},
   "source": [
    "### Synsets are organized in an inheritance tree like structure in which Hypernyms represents more abstracted terms while Hyponyms represents the more specific terms.One of the important things is that this tree can be traced all the way to a root hypernym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c46a65f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernyms() #canine and domestic animal are the hypernyms of the 'dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ed8f6494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('bitch.n.04'),\n",
       " Synset('dog.n.01'),\n",
       " Synset('fox.n.01'),\n",
       " Synset('hyena.n.01'),\n",
       " Synset('jackal.n.01'),\n",
       " Synset('wild_dog.n.01'),\n",
       " Synset('wolf.n.01')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernyms()[0].hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d3e2d8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.root_hypernyms() #to get root of hyponyms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c844ceef",
   "metadata": {},
   "source": [
    "### Lemmas\n",
    "   #### Canonical form or morphological form of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1ab7cf80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('dog.n.01.dog'),\n",
       " Lemma('dog.n.01.domestic_dog'),\n",
       " Lemma('dog.n.01.Canis_familiaris')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6cc56e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma=syn.lemmas()\n",
    "len(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "773d2717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'evil'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to get the antonym of the word\n",
    "syn1 = wn.synset('good.n.02')\n",
    "antonym1 = syn1.lemmas()[0].antonyms()[0]\n",
    "antonym1.name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59ece71",
   "metadata": {},
   "source": [
    "# Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f5c8a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pattern\n",
      "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
      "                                              0.0/22.2 MB ? eta -:--:--\n",
      "                                             0.0/22.2 MB 667.8 kB/s eta 0:00:34\n",
      "                                              0.1/22.2 MB 1.4 MB/s eta 0:00:16\n",
      "                                              0.2/22.2 MB 1.4 MB/s eta 0:00:16\n",
      "                                              0.3/22.2 MB 1.3 MB/s eta 0:00:17\n",
      "                                              0.4/22.2 MB 1.8 MB/s eta 0:00:13\n",
      "                                              0.5/22.2 MB 2.0 MB/s eta 0:00:12\n",
      "     -                                        0.7/22.2 MB 2.0 MB/s eta 0:00:11\n",
      "     -                                        0.9/22.2 MB 2.5 MB/s eta 0:00:09\n",
      "     -                                        1.1/22.2 MB 2.6 MB/s eta 0:00:09\n",
      "     --                                       1.2/22.2 MB 2.6 MB/s eta 0:00:09\n",
      "     --                                       1.4/22.2 MB 2.7 MB/s eta 0:00:08\n",
      "     --                                       1.6/22.2 MB 2.8 MB/s eta 0:00:08\n",
      "     ---                                      1.8/22.2 MB 2.9 MB/s eta 0:00:07\n",
      "     ---                                      2.0/22.2 MB 3.1 MB/s eta 0:00:07\n",
      "     ---                                      2.2/22.2 MB 3.1 MB/s eta 0:00:07\n",
      "     ----                                     2.4/22.2 MB 3.1 MB/s eta 0:00:07\n",
      "     ----                                     2.5/22.2 MB 3.2 MB/s eta 0:00:07\n",
      "     ----                                     2.7/22.2 MB 3.2 MB/s eta 0:00:07\n",
      "     -----                                    2.8/22.2 MB 3.2 MB/s eta 0:00:07\n",
      "     -----                                    3.0/22.2 MB 3.2 MB/s eta 0:00:06\n",
      "     -----                                    3.1/22.2 MB 3.1 MB/s eta 0:00:07\n",
      "     -----                                    3.2/22.2 MB 3.1 MB/s eta 0:00:07\n",
      "     -----                                    3.3/22.2 MB 3.1 MB/s eta 0:00:07\n",
      "     ------                                   3.5/22.2 MB 3.1 MB/s eta 0:00:07\n",
      "     ------                                   3.7/22.2 MB 3.1 MB/s eta 0:00:07\n",
      "     ------                                   3.8/22.2 MB 3.1 MB/s eta 0:00:07\n",
      "     -------                                  4.0/22.2 MB 3.0 MB/s eta 0:00:07\n",
      "     -------                                  4.2/22.2 MB 3.1 MB/s eta 0:00:06\n",
      "     -------                                  4.3/22.2 MB 3.2 MB/s eta 0:00:06\n",
      "     --------                                 4.5/22.2 MB 3.1 MB/s eta 0:00:06\n",
      "     --------                                 4.6/22.2 MB 3.1 MB/s eta 0:00:06\n",
      "     --------                                 4.7/22.2 MB 3.1 MB/s eta 0:00:06\n",
      "     --------                                 5.0/22.2 MB 3.1 MB/s eta 0:00:06\n",
      "     ---------                                5.1/22.2 MB 3.1 MB/s eta 0:00:06\n",
      "     ---------                                5.3/22.2 MB 3.2 MB/s eta 0:00:06\n",
      "     ---------                                5.4/22.2 MB 3.2 MB/s eta 0:00:06\n",
      "     ---------                                5.5/22.2 MB 3.2 MB/s eta 0:00:06\n",
      "     ----------                               5.7/22.2 MB 3.2 MB/s eta 0:00:06\n",
      "     ----------                               5.8/22.2 MB 3.1 MB/s eta 0:00:06\n",
      "     ----------                               6.0/22.2 MB 3.1 MB/s eta 0:00:06\n",
      "     ----------                               6.1/22.2 MB 3.1 MB/s eta 0:00:06\n",
      "     -----------                              6.2/22.2 MB 3.1 MB/s eta 0:00:06\n",
      "     -----------                              6.3/22.2 MB 3.1 MB/s eta 0:00:06\n",
      "     -----------                              6.5/22.2 MB 3.1 MB/s eta 0:00:06\n",
      "     -----------                              6.6/22.2 MB 3.0 MB/s eta 0:00:06\n",
      "     ------------                             6.7/22.2 MB 3.0 MB/s eta 0:00:06\n",
      "     ------------                             6.8/22.2 MB 3.0 MB/s eta 0:00:06\n",
      "     ------------                             6.9/22.2 MB 3.0 MB/s eta 0:00:06\n",
      "     ------------                             7.0/22.2 MB 3.0 MB/s eta 0:00:06\n",
      "     ------------                             7.1/22.2 MB 3.0 MB/s eta 0:00:06\n",
      "     -------------                            7.2/22.2 MB 3.0 MB/s eta 0:00:06\n",
      "     -------------                            7.4/22.2 MB 3.0 MB/s eta 0:00:06\n",
      "     -------------                            7.4/22.2 MB 2.9 MB/s eta 0:00:06\n",
      "     -------------                            7.6/22.2 MB 3.0 MB/s eta 0:00:05\n",
      "     --------------                           7.8/22.2 MB 3.0 MB/s eta 0:00:05\n",
      "     --------------                           8.0/22.2 MB 3.0 MB/s eta 0:00:05\n",
      "     --------------                           8.2/22.2 MB 3.0 MB/s eta 0:00:05\n",
      "     ---------------                          8.4/22.2 MB 3.0 MB/s eta 0:00:05\n",
      "     ---------------                          8.5/22.2 MB 3.0 MB/s eta 0:00:05\n",
      "     ---------------                          8.7/22.2 MB 3.0 MB/s eta 0:00:05\n",
      "     ---------------                          8.7/22.2 MB 3.0 MB/s eta 0:00:05\n",
      "     ---------------                          8.8/22.2 MB 3.0 MB/s eta 0:00:05\n",
      "     ---------------                          8.9/22.2 MB 2.9 MB/s eta 0:00:05\n",
      "     ----------------                         8.9/22.2 MB 2.9 MB/s eta 0:00:05\n",
      "     ----------------                         9.0/22.2 MB 2.9 MB/s eta 0:00:05\n",
      "     ----------------                         9.1/22.2 MB 2.9 MB/s eta 0:00:05\n",
      "     ----------------                         9.2/22.2 MB 2.9 MB/s eta 0:00:05\n",
      "     ----------------                         9.3/22.2 MB 2.9 MB/s eta 0:00:05\n",
      "     -----------------                        9.5/22.2 MB 2.9 MB/s eta 0:00:05\n",
      "     -----------------                        9.6/22.2 MB 2.9 MB/s eta 0:00:05\n",
      "     -----------------                        9.7/22.2 MB 2.9 MB/s eta 0:00:05\n",
      "     -----------------                        9.8/22.2 MB 2.9 MB/s eta 0:00:05\n",
      "     -----------------                        9.9/22.2 MB 2.8 MB/s eta 0:00:05\n",
      "     ------------------                       10.0/22.2 MB 2.8 MB/s eta 0:00:05\n",
      "     ------------------                       10.1/22.2 MB 2.8 MB/s eta 0:00:05\n",
      "     ------------------                       10.2/22.2 MB 2.8 MB/s eta 0:00:05\n",
      "     ------------------                       10.3/22.2 MB 2.8 MB/s eta 0:00:05\n",
      "     ------------------                       10.4/22.2 MB 2.8 MB/s eta 0:00:05\n",
      "     ------------------                       10.5/22.2 MB 2.9 MB/s eta 0:00:05\n",
      "     -------------------                      10.6/22.2 MB 2.8 MB/s eta 0:00:05\n",
      "     -------------------                      10.7/22.2 MB 2.8 MB/s eta 0:00:05\n",
      "     -------------------                      10.8/22.2 MB 2.8 MB/s eta 0:00:05\n",
      "     -------------------                      10.9/22.2 MB 2.8 MB/s eta 0:00:05\n",
      "     -------------------                      11.0/22.2 MB 2.8 MB/s eta 0:00:05\n",
      "     -------------------                      11.1/22.2 MB 2.8 MB/s eta 0:00:04\n",
      "     --------------------                     11.2/22.2 MB 2.8 MB/s eta 0:00:04\n",
      "     --------------------                     11.4/22.2 MB 2.8 MB/s eta 0:00:04\n",
      "     --------------------                     11.5/22.2 MB 2.8 MB/s eta 0:00:04\n",
      "     --------------------                     11.6/22.2 MB 2.7 MB/s eta 0:00:04\n",
      "     --------------------                     11.6/22.2 MB 2.7 MB/s eta 0:00:04\n",
      "     ---------------------                    11.8/22.2 MB 2.7 MB/s eta 0:00:04\n",
      "     ---------------------                    12.0/22.2 MB 2.7 MB/s eta 0:00:04\n",
      "     ---------------------                    12.1/22.2 MB 2.7 MB/s eta 0:00:04\n",
      "     ----------------------                   12.3/22.2 MB 2.7 MB/s eta 0:00:04\n",
      "     ----------------------                   12.4/22.2 MB 2.7 MB/s eta 0:00:04\n",
      "     ----------------------                   12.5/22.2 MB 2.7 MB/s eta 0:00:04\n",
      "     ----------------------                   12.6/22.2 MB 2.6 MB/s eta 0:00:04\n",
      "     ----------------------                   12.8/22.2 MB 2.6 MB/s eta 0:00:04\n",
      "     -----------------------                  12.9/22.2 MB 2.6 MB/s eta 0:00:04\n",
      "     -----------------------                  13.0/22.2 MB 2.6 MB/s eta 0:00:04\n",
      "     -----------------------                  13.2/22.2 MB 2.6 MB/s eta 0:00:04\n",
      "     ------------------------                 13.5/22.2 MB 2.7 MB/s eta 0:00:04\n",
      "     ------------------------                 13.6/22.2 MB 2.7 MB/s eta 0:00:04\n",
      "     ------------------------                 13.7/22.2 MB 2.7 MB/s eta 0:00:04\n",
      "     ------------------------                 13.8/22.2 MB 2.7 MB/s eta 0:00:04\n",
      "     -------------------------                14.0/22.2 MB 2.7 MB/s eta 0:00:04\n",
      "     -------------------------                14.2/22.2 MB 2.7 MB/s eta 0:00:04\n",
      "     -------------------------                14.2/22.2 MB 2.6 MB/s eta 0:00:04\n",
      "     -------------------------                14.4/22.2 MB 2.6 MB/s eta 0:00:03\n",
      "     --------------------------               14.6/22.2 MB 2.6 MB/s eta 0:00:03\n",
      "     --------------------------               14.7/22.2 MB 2.6 MB/s eta 0:00:03\n",
      "     --------------------------               14.8/22.2 MB 2.6 MB/s eta 0:00:03\n",
      "     --------------------------               14.9/22.2 MB 2.6 MB/s eta 0:00:03\n",
      "     ---------------------------              15.1/22.2 MB 2.6 MB/s eta 0:00:03\n",
      "     ---------------------------              15.1/22.2 MB 2.6 MB/s eta 0:00:03\n",
      "     ---------------------------              15.3/22.2 MB 2.6 MB/s eta 0:00:03\n",
      "     ---------------------------              15.4/22.2 MB 2.6 MB/s eta 0:00:03\n",
      "     ---------------------------              15.5/22.2 MB 2.5 MB/s eta 0:00:03\n",
      "     ----------------------------             15.6/22.2 MB 2.5 MB/s eta 0:00:03\n",
      "     ----------------------------             15.6/22.2 MB 2.5 MB/s eta 0:00:03\n",
      "     ----------------------------             15.7/22.2 MB 2.5 MB/s eta 0:00:03\n",
      "     ----------------------------             15.8/22.2 MB 2.5 MB/s eta 0:00:03\n",
      "     ----------------------------             15.9/22.2 MB 2.5 MB/s eta 0:00:03\n",
      "     ----------------------------             16.0/22.2 MB 2.5 MB/s eta 0:00:03\n",
      "     ----------------------------             16.1/22.2 MB 2.5 MB/s eta 0:00:03\n",
      "     -----------------------------            16.2/22.2 MB 2.5 MB/s eta 0:00:03\n",
      "     -----------------------------            16.3/22.2 MB 2.5 MB/s eta 0:00:03\n",
      "     -----------------------------            16.4/22.2 MB 2.5 MB/s eta 0:00:03\n",
      "     -----------------------------            16.6/22.2 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------------           16.7/22.2 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------------           16.9/22.2 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------------           17.0/22.2 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------------           17.2/22.2 MB 2.5 MB/s eta 0:00:03\n",
      "     -------------------------------          17.4/22.2 MB 2.5 MB/s eta 0:00:02\n",
      "     -------------------------------          17.6/22.2 MB 2.6 MB/s eta 0:00:02\n",
      "     -------------------------------          17.8/22.2 MB 2.6 MB/s eta 0:00:02\n",
      "     --------------------------------         18.0/22.2 MB 2.6 MB/s eta 0:00:02\n",
      "     --------------------------------         18.2/22.2 MB 2.6 MB/s eta 0:00:02\n",
      "     ---------------------------------        18.4/22.2 MB 2.6 MB/s eta 0:00:02\n",
      "     ---------------------------------        18.5/22.2 MB 2.6 MB/s eta 0:00:02\n",
      "     ---------------------------------        18.7/22.2 MB 2.6 MB/s eta 0:00:02\n",
      "     ---------------------------------        18.9/22.2 MB 2.6 MB/s eta 0:00:02\n",
      "     ----------------------------------       19.0/22.2 MB 2.6 MB/s eta 0:00:02\n",
      "     ----------------------------------       19.1/22.2 MB 2.6 MB/s eta 0:00:02\n",
      "     ----------------------------------       19.3/22.2 MB 2.7 MB/s eta 0:00:02\n",
      "     ----------------------------------       19.4/22.2 MB 2.7 MB/s eta 0:00:02\n",
      "     ----------------------------------       19.4/22.2 MB 2.7 MB/s eta 0:00:02\n",
      "     ----------------------------------       19.4/22.2 MB 2.7 MB/s eta 0:00:02\n",
      "     ----------------------------------       19.4/22.2 MB 2.7 MB/s eta 0:00:02\n",
      "     -----------------------------------      19.8/22.2 MB 2.6 MB/s eta 0:00:01\n",
      "     -----------------------------------      19.9/22.2 MB 2.6 MB/s eta 0:00:01\n",
      "     -----------------------------------      20.0/22.2 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------------------------------     20.2/22.2 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------------------------------     20.2/22.2 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------------------------------     20.3/22.2 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------------------------------     20.4/22.2 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------------------------------     20.5/22.2 MB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------------------    20.6/22.2 MB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------------------    20.7/22.2 MB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------------------    20.8/22.2 MB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------------------    20.9/22.2 MB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------------------    21.0/22.2 MB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------------------    21.1/22.2 MB 2.6 MB/s eta 0:00:01\n",
      "     --------------------------------------   21.1/22.2 MB 2.6 MB/s eta 0:00:01\n",
      "     --------------------------------------   21.3/22.2 MB 2.6 MB/s eta 0:00:01\n",
      "     --------------------------------------   21.4/22.2 MB 2.6 MB/s eta 0:00:01\n",
      "     --------------------------------------   21.4/22.2 MB 2.6 MB/s eta 0:00:01\n",
      "     --------------------------------------   21.5/22.2 MB 2.5 MB/s eta 0:00:01\n",
      "     --------------------------------------   21.5/22.2 MB 2.5 MB/s eta 0:00:01\n",
      "     --------------------------------------   21.6/22.2 MB 2.5 MB/s eta 0:00:01\n",
      "     --------------------------------------   21.7/22.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  21.8/22.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  21.9/22.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  22.0/22.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  22.1/22.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  22.2/22.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  22.2/22.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  22.2/22.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  22.2/22.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  22.2/22.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  22.2/22.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  22.2/22.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  22.2/22.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 22.2/22.2 MB 2.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: future in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from pattern) (0.18.3)\n",
      "Collecting backports.csv (from pattern)\n",
      "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
      "Collecting mysqlclient (from pattern)\n",
      "  Downloading mysqlclient-2.2.1-cp311-cp311-win_amd64.whl (202 kB)\n",
      "                                              0.0/202.8 kB ? eta -:--:--\n",
      "     -----------------------                122.9/202.8 kB 3.5 MB/s eta 0:00:01\n",
      "     ------------------------------------   194.6/202.8 kB 2.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 202.8/202.8 kB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from pattern) (4.12.2)\n",
      "Requirement already satisfied: lxml in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from pattern) (4.9.2)\n",
      "Collecting feedparser (from pattern)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "                                              0.0/81.3 kB ? eta -:--:--\n",
      "                                              0.0/81.3 kB ? eta -:--:--\n",
      "                                              0.0/81.3 kB ? eta -:--:--\n",
      "     -----------------------------------      71.7/81.3 kB ? eta -:--:--\n",
      "     -----------------------------------      71.7/81.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 81.3/81.3 kB 911.6 kB/s eta 0:00:00\n",
      "Collecting pdfminer.six (from pattern)\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "                                              0.0/5.6 MB ? eta -:--:--\n",
      "     -                                        0.2/5.6 MB 5.3 MB/s eta 0:00:02\n",
      "     --                                       0.4/5.6 MB 4.9 MB/s eta 0:00:02\n",
      "     ----                                     0.6/5.6 MB 4.8 MB/s eta 0:00:02\n",
      "     -----                                    0.8/5.6 MB 4.1 MB/s eta 0:00:02\n",
      "     ------                                   1.0/5.6 MB 4.4 MB/s eta 0:00:02\n",
      "     -------                                  1.1/5.6 MB 4.1 MB/s eta 0:00:02\n",
      "     ---------                                1.3/5.6 MB 4.0 MB/s eta 0:00:02\n",
      "     ---------                                1.4/5.6 MB 3.9 MB/s eta 0:00:02\n",
      "     ----------                               1.5/5.6 MB 3.6 MB/s eta 0:00:02\n",
      "     -----------                              1.6/5.6 MB 3.6 MB/s eta 0:00:02\n",
      "     -------------                            1.9/5.6 MB 3.6 MB/s eta 0:00:02\n",
      "     --------------                           2.0/5.6 MB 3.7 MB/s eta 0:00:01\n",
      "     ---------------                          2.2/5.6 MB 3.5 MB/s eta 0:00:01\n",
      "     -----------------                        2.5/5.6 MB 3.7 MB/s eta 0:00:01\n",
      "     ------------------                       2.7/5.6 MB 3.7 MB/s eta 0:00:01\n",
      "     --------------------                     2.8/5.6 MB 3.7 MB/s eta 0:00:01\n",
      "     ---------------------                    3.1/5.6 MB 3.7 MB/s eta 0:00:01\n",
      "     ----------------------                   3.2/5.6 MB 3.7 MB/s eta 0:00:01\n",
      "     ------------------------                 3.4/5.6 MB 3.7 MB/s eta 0:00:01\n",
      "     ------------------------                 3.5/5.6 MB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------                3.6/5.6 MB 3.6 MB/s eta 0:00:01\n",
      "     --------------------------               3.8/5.6 MB 3.7 MB/s eta 0:00:01\n",
      "     ----------------------------             4.0/5.6 MB 3.6 MB/s eta 0:00:01\n",
      "     -----------------------------            4.2/5.6 MB 3.6 MB/s eta 0:00:01\n",
      "     ------------------------------           4.3/5.6 MB 3.6 MB/s eta 0:00:01\n",
      "     --------------------------------         4.5/5.6 MB 3.6 MB/s eta 0:00:01\n",
      "     ---------------------------------        4.7/5.6 MB 3.6 MB/s eta 0:00:01\n",
      "     ----------------------------------       4.8/5.6 MB 3.6 MB/s eta 0:00:01\n",
      "     -----------------------------------      4.9/5.6 MB 3.6 MB/s eta 0:00:01\n",
      "     ------------------------------------     5.1/5.6 MB 3.6 MB/s eta 0:00:01\n",
      "     -------------------------------------    5.3/5.6 MB 3.6 MB/s eta 0:00:01\n",
      "     --------------------------------------   5.4/5.6 MB 3.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.5/5.6 MB 3.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.6/5.6 MB 3.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.6/5.6 MB 3.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.6/5.6 MB 3.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.6/5.6 MB 3.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.6/5.6 MB 3.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 5.6/5.6 MB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from pattern) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from pattern) (1.10.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from pattern) (3.7)\n",
      "Collecting python-docx (from pattern)\n",
      "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
      "                                              0.0/239.6 kB ? eta -:--:--\n",
      "     ------------------------------         194.6/239.6 kB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  235.5/239.6 kB 3.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 239.6/239.6 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting cherrypy (from pattern)\n",
      "  Downloading CherryPy-18.9.0-py3-none-any.whl (348 kB)\n",
      "                                              0.0/348.8 kB ? eta -:--:--\n",
      "     ------------------------               225.3/348.8 kB 6.7 MB/s eta 0:00:01\n",
      "     -------------------------------------  348.2/348.8 kB 3.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 348.8/348.8 kB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from pattern) (2.29.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from beautifulsoup4->pattern) (2.4)\n",
      "Collecting cheroot>=8.2.1 (from cherrypy->pattern)\n",
      "  Downloading cheroot-10.0.0-py3-none-any.whl (101 kB)\n",
      "                                              0.0/101.6 kB ? eta -:--:--\n",
      "     ------------------------------------     92.2/101.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 101.6/101.6 kB 1.9 MB/s eta 0:00:00\n",
      "Collecting portend>=2.1.1 (from cherrypy->pattern)\n",
      "  Downloading portend-3.2.0-py3-none-any.whl (5.3 kB)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from cherrypy->pattern) (8.12.0)\n",
      "Collecting zc.lockfile (from cherrypy->pattern)\n",
      "  Downloading zc.lockfile-3.0.post1-py3-none-any.whl (9.8 kB)\n",
      "Collecting jaraco.collections (from cherrypy->pattern)\n",
      "  Downloading jaraco.collections-5.0.0-py3-none-any.whl (10 kB)\n",
      "Collecting sgmllib3k (from feedparser->pattern)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: click in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from nltk->pattern) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from nltk->pattern) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from nltk->pattern) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from nltk->pattern) (4.65.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from pdfminer.six->pattern) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from pdfminer.six->pattern) (39.0.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from python-docx->pattern) (4.6.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from requests->pattern) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from requests->pattern) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from requests->pattern) (2023.5.7)\n",
      "Collecting jaraco.functools (from cheroot>=8.2.1->cherrypy->pattern)\n",
      "  Downloading jaraco.functools-4.0.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.15.1)\n",
      "Collecting tempora>=1.8 (from portend>=2.1.1->cherrypy->pattern)\n",
      "  Downloading tempora-5.5.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from click->nltk->pattern) (0.4.6)\n",
      "Collecting jaraco.text (from jaraco.collections->cherrypy->pattern)\n",
      "  Downloading jaraco.text-3.12.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from zc.lockfile->cherrypy->pattern) (67.8.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.21)\n",
      "Requirement already satisfied: pytz in c:\\users\\shahana s\\anaconda3\\lib\\site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2022.7)\n",
      "Collecting jaraco.context>=4.1 (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
      "  Downloading jaraco.context-4.3.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting autocommand (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
      "  Downloading autocommand-2.2.2-py3-none-any.whl (19 kB)\n",
      "Collecting inflect (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
      "  Downloading inflect-7.0.0-py3-none-any.whl (34 kB)\n",
      "Collecting pydantic>=1.9.1 (from inflect->jaraco.text->jaraco.collections->cherrypy->pattern)\n",
      "  Downloading pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
      "                                              0.0/381.9 kB ? eta -:--:--\n",
      "     -----------                            112.6/381.9 kB 2.2 MB/s eta 0:00:01\n",
      "     ----------------------                 225.3/381.9 kB 2.8 MB/s eta 0:00:01\n",
      "     -------------------------------        317.4/381.9 kB 2.5 MB/s eta 0:00:01\n",
      "     -------------------------------------  378.9/381.9 kB 2.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 381.9/381.9 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Collecting pydantic-core==2.14.6 (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern)\n",
      "  Downloading pydantic_core-2.14.6-cp311-none-win_amd64.whl (1.9 MB)\n",
      "                                              0.0/1.9 MB ? eta -:--:--\n",
      "     -                                        0.1/1.9 MB 2.2 MB/s eta 0:00:01\n",
      "     -----                                    0.2/1.9 MB 3.0 MB/s eta 0:00:01\n",
      "     --------                                 0.4/1.9 MB 2.7 MB/s eta 0:00:01\n",
      "     ----------                               0.5/1.9 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------                             0.6/1.9 MB 2.7 MB/s eta 0:00:01\n",
      "     ----------------                         0.8/1.9 MB 2.7 MB/s eta 0:00:01\n",
      "     ------------------                       0.9/1.9 MB 2.6 MB/s eta 0:00:01\n",
      "     --------------------                     1.0/1.9 MB 2.5 MB/s eta 0:00:01\n",
      "     ------------------------                 1.1/1.9 MB 2.7 MB/s eta 0:00:01\n",
      "     --------------------------               1.3/1.9 MB 2.6 MB/s eta 0:00:01\n",
      "     -----------------------------            1.4/1.9 MB 2.7 MB/s eta 0:00:01\n",
      "     --------------------------------         1.5/1.9 MB 2.6 MB/s eta 0:00:01\n",
      "     -----------------------------------      1.7/1.9 MB 2.7 MB/s eta 0:00:01\n",
      "     --------------------------------------   1.8/1.9 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.9/1.9 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.9/1.9 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.9/1.9 MB 2.4 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: pattern, sgmllib3k\n",
      "  Building wheel for pattern (setup.py): started\n",
      "  Building wheel for pattern (setup.py): finished with status 'done'\n",
      "  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332714 sha256=1109d3c89e61f504700a94bf5faabf962c6fdc4a3b63d45075f149bf9b600f71\n",
      "  Stored in directory: c:\\users\\shahana s\\appdata\\local\\pip\\cache\\wheels\\d0\\8d\\e1\\44b5ab952791ca07e66a780d11dbf141c49d151f0be21e811b\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6061 sha256=fd12c90b3d517042cca6112213e5ae452ed54cfe9a897bb67dffdf4686da4fa9\n",
      "  Stored in directory: c:\\users\\shahana s\\appdata\\local\\pip\\cache\\wheels\\3b\\25\\2a\\105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
      "Successfully built pattern sgmllib3k\n",
      "Installing collected packages: sgmllib3k, backports.csv, zc.lockfile, python-docx, pydantic-core, mysqlclient, jaraco.functools, jaraco.context, feedparser, autocommand, annotated-types, tempora, pydantic, cheroot, portend, pdfminer.six, inflect, jaraco.text, jaraco.collections, cherrypy, pattern\n",
      "Successfully installed annotated-types-0.6.0 autocommand-2.2.2 backports.csv-1.0.7 cheroot-10.0.0 cherrypy-18.9.0 feedparser-6.0.11 inflect-7.0.0 jaraco.collections-5.0.0 jaraco.context-4.3.0 jaraco.functools-4.0.0 jaraco.text-3.12.0 mysqlclient-2.2.1 pattern-3.6 pdfminer.six-20231228 portend-3.2.0 pydantic-2.5.3 pydantic-core-2.14.6 python-docx-1.1.0 sgmllib3k-1.0.0 tempora-5.5.0 zc.lockfile-3.0.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2163864d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Shahana\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6946552d",
   "metadata": {},
   "source": [
    "## What is lexeme and example?\n",
    "### A lexeme is a basic unit of meaning. Lexemes are the headwords in dictionaries. The lexeme “play,” for example, can take many forms, such as playing, plays, played. A lexicon consists of lexemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0fc39700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rent', 'rents', 'renting', 'rented']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pattern\n",
    "from pattern.en import lexeme\n",
    "lexeme(\"rent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5844f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
